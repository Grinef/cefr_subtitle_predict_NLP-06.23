{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aah_A1lCLttJ"
   },
   "source": [
    "## **Проект \"English Score\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_7vwpbWL751"
   },
   "source": [
    "Цель проекта: создать модель классификации фильмов по уровню английского языка.\n",
    "\n",
    "\n",
    "Задачи проекта:\n",
    "\n",
    "1. Загрузить данные (расширить дата-сет, если это возможно);\n",
    "2. Преобразовать и почистить данные;\n",
    "3. Подготовить данные к моделированию;\n",
    "4. Моделирование;\n",
    "5. Оценка качества моделирования.\n",
    "\n",
    "Автор:\n",
    "\n",
    "* ФИО - Рожнятовский Г.И.\n",
    "* email - grinef00@yandex.ru\n",
    "* telegram - @grinef\n",
    "\n",
    "\n",
    "Дополнительный комментарий:\n",
    "* Полностью код выполняется в течение 15 минут.\n",
    "* на данный момент представлена первая версия кода."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEqPxctP41OA"
   },
   "source": [
    "### **Импорты**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SD57LtgHKrFJ"
   },
   "outputs": [],
   "source": [
    "# для загрузки данных\n",
    "import os\n",
    "import camelot\n",
    "import ghostscript\n",
    "import pysrt\n",
    "import re\n",
    "\n",
    "# для анализа данных:\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# предобработка\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Grine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Загрузка списка стоп-слов\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Grine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузка предварительно обученной модели POS-теггинга\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Grine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# для лемматизации\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAzpi3ddz25v"
   },
   "source": [
    "### **Загрузка данных**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYEdKE1shhXW"
   },
   "source": [
    "#### **Загрузка и подготовка словарей**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "id": "LHX1UhpGNrvs",
    "outputId": "23059f1a-db7d-48d8-eb1d-31c560fb7aa0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# загрузим сначала все слова, и распределим по уровню\n",
    "directory = r'C:\\Users\\Grine\\Desktop\\GitHub\\English_level_predict\\Oxford_CEFR_level'\n",
    "\n",
    "# Получение списка всех файлов в директории\n",
    "file_names = os.listdir(directory)\n",
    "\n",
    "cefr_vocabulary = {\n",
    "    'a1': [],\n",
    "    'a2': [],\n",
    "    'b1': [],\n",
    "    'b2': [],\n",
    "    'c1': []\n",
    "    } # создадим словарь, в который внесём все уровни слов\n",
    "\n",
    "# прочитаем все файлы в папке и сгенерируем путь\n",
    "\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(directory, file_name)      # создадим пути к всем pdf-файлам\n",
    "    \n",
    "    tables = camelot.read_pdf(file_path,\n",
    "                              pages='1-end',\n",
    "                              flavor='stream',\n",
    "                              strip_text='\\n')          # создадим список всех таблиц\n",
    "    engish_level = None\n",
    "    for table in tables:                                # для каждой таблицы\n",
    "        target_object = table.df.reset_index(drop=True) # обновим индексы\n",
    "        \n",
    "        for c_n in target_object.columns:               # и выделим названия столбцов\n",
    "            for v in target_object[c_n]:                # пройдёмся циклом по каждому значению каждого столбца\n",
    "                \n",
    "                value = v.lower()\n",
    "                if value in cefr_vocabulary.keys():      # если значение столбца есть в ключах словаря\n",
    "                    engish_level = value                 # то присвоим переменной это значение\n",
    "                    \n",
    "                # добавим значение в словарь\n",
    "                try:\n",
    "                    cefr_vocabulary[engish_level].append(value.split(' ')[0]) if value != '' else None\n",
    "                except KeyError: # если ключа нет - продолжим. \n",
    "                    continue      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vDxZg1O8-TFt",
    "outputId": "fb4f07f2-1341-4f33-f05b-6ab7ada86e3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "в категории, a1, всего 1820 слов\n",
      "в категории, a2, всего 1694 слов\n",
      "в категории, b1, всего 1700 слов\n",
      "в категории, b2, всего 2888 слов\n",
      "в категории, c1, всего 2657 слов\n",
      "Всего слов отобрано: 10759\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in cefr_vocabulary.keys():\n",
    "    amount = len(cefr_vocabulary[i])\n",
    "    count += amount\n",
    "    print(f'в категории, {i}, всего {amount} слов')\n",
    "\n",
    "print(f'Всего слов отобрано: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyMNutFJhoLt"
   },
   "source": [
    "#### **Загрузка данных для  анализа**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LrTYAoidhvCJ"
   },
   "outputs": [],
   "source": [
    "subtitle_path = r'C:\\Users\\Grine\\Desktop\\GitHub\\English_level_predict\\english_scores' # путь к файлу, где лежат данные\n",
    "\n",
    "# Получение списка всех групп-файлов в директории\n",
    "subtitle_group = os.listdir(subtitle_path) \n",
    "\n",
    "# загрузим размеченные фильмы \n",
    "movie_labels = pd.read_excel(subtitle_path + '\\\\' + subtitle_group[0]).iloc[:, 1:]\n",
    "\n",
    "# преобразуем названия столбцов\n",
    "movie_labels.columns = [column_name.lower() for column_name in movie_labels.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iLJ9aorciE4O",
    "outputId": "f43cc744-5296-4e40-e0ab-a1992ec7d263"
   },
   "outputs": [],
   "source": [
    "# переинециализируем путь и снова получим список групп\n",
    "subtitle_path = subtitle_path + '\\\\' + subtitle_group[1]\n",
    "subtitle_group_labels = os.listdir(subtitle_path)[1:] # первый файл '.DS_store' исключим его из дальнейшего парсера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for sub_group in subtitle_group_labels:                                               # для каждой группы папок\n",
    "    path_files = subtitle_path + '\\\\' + sub_group                                     # инициализируем новый путь\n",
    "    files = os.listdir(path_files)                                                    # получим список файлов\n",
    "    for file_name in files:                                                           # прочтём каждый файл\n",
    "        df_step = pd.DataFrame({                                                      \n",
    "          'label':  [sub_group],\n",
    "          'name': [file_name.split('.s')[0]],\n",
    "          'subtitle': [pysrt.open(path_files + '\\\\' + file_name, encoding='latin-1').text]\n",
    "        }) #  внесём доступные данные о дата-фрейме в нашу таблицу\n",
    "        \n",
    "        df = pd.concat([df, df_step]) # соединим результаты\n",
    "        \n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# далее, попробуем соединить те фильмы, которые храняться в переменной: \"movie_labels\" и \"df\"\n",
    "# Но перед тем как это сделать, стоит преобразовать данные.\n",
    "def base_text_transform(data, column) -> Series:\n",
    "    '''\n",
    "    На входе функция получает данные (data) и столбец, которые требует преобразования (column)\n",
    "    На выходе функция возвращает предобработанный столбец, в котором храняться только цифры, буквы и знак пробела -> \"_\"\n",
    "    '''\n",
    "    array = data[column].values # преобразуем в массив\n",
    "    array_lower = [mean.lower() for mean in array] # приведём массив к нижнему регистру\n",
    "    array_lower_clean = [re.sub(r'[^a-zA-Z0-9_\\s]', '', mean) for mean in array_lower] # оставим только буквы, цифры и пробелы\n",
    "    array_lower_clean_space = [mean.replace(' ', '_') for mean in array_lower_clean]   # преобразуем пробелы на нижние подчёркивания\n",
    "    return pd.Series(array_lower_clean_space) # вернём очищенный столбец"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['name_to_merge'] = base_text_transform(df, 'name')                      # преобразуем таблицу с субтитрами\n",
    "movie_labels['name_to_merge'] = base_text_transform(movie_labels, 'movie') # преобразуем таблицу с метками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = movie_labels.merge(df, how='outer') # создадим дата-фрейм, включающий все записи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Подготовка и изучение данных**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Изучение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>level</th>\n",
       "      <th>name_to_merge</th>\n",
       "      <th>label</th>\n",
       "      <th>name</th>\n",
       "      <th>subtitle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>B1</td>\n",
       "      <td>10_cloverfield_lane2016</td>\n",
       "      <td>Subtitles</td>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>&lt;font color=\"#ffff80\"&gt;&lt;b&gt;Fixed &amp; Synced by boz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>B1</td>\n",
       "      <td>10_things_i_hate_about_you1999</td>\n",
       "      <td>Subtitles</td>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>Hey!\\nI'll be right with you.\\nSo, Cameron. He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>B2</td>\n",
       "      <td>a_knights_tale2001</td>\n",
       "      <td>Subtitles</td>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>Resync: Xenzai[NEF]\\nRETAIL\\nShould we help hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>B2</td>\n",
       "      <td>a_star_is_born2018</td>\n",
       "      <td>Subtitles</td>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>- &lt;i&gt;&lt;font color=\"#ffffff\"&gt; Synced and correct...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              movie level                   name_to_merge  \\\n",
       "0         10_Cloverfield_lane(2016)    B1         10_cloverfield_lane2016   \n",
       "1  10_things_I_hate_about_you(1999)    B1  10_things_i_hate_about_you1999   \n",
       "2              A_knights_tale(2001)    B2              a_knights_tale2001   \n",
       "3              A_star_is_born(2018)    B2              a_star_is_born2018   \n",
       "\n",
       "       label                              name  \\\n",
       "0  Subtitles         10_Cloverfield_lane(2016)   \n",
       "1  Subtitles  10_things_I_hate_about_you(1999)   \n",
       "2  Subtitles              A_knights_tale(2001)   \n",
       "3  Subtitles              A_star_is_born(2018)   \n",
       "\n",
       "                                            subtitle  \n",
       "0  <font color=\"#ffff80\"><b>Fixed & Synced by boz...  \n",
       "1  Hey!\\nI'll be right with you.\\nSo, Cameron. He...  \n",
       "2  Resync: Xenzai[NEF]\\nRETAIL\\nShould we help hi...  \n",
       "3  - <i><font color=\"#ffffff\"> Synced and correct...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.head(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 292 entries, 0 to 291\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   movie          241 non-null    object\n",
      " 1   level          241 non-null    object\n",
      " 2   name_to_merge  292 non-null    object\n",
      " 3   label          283 non-null    object\n",
      " 4   name           283 non-null    object\n",
      " 5   subtitle       283 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 13.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_full.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movie            51\n",
       "level            51\n",
       "name_to_merge     0\n",
       "label             9\n",
       "name              9\n",
       "subtitle          9\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# изучим пропуски\n",
    "df_full.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Массив уникальных названий, которые есть в таблице df: \n",
      " ['SlingShot (2014) WEB.eng' 'Crown, The S01E01 - Wolferton Splash.en.SDH'\n",
      " 'Crown, The S01E01 - Wolferton Splash.en'\n",
      " 'Crown, The S01E02 - Hyde Park Corner.en.SDH'\n",
      " 'Crown, The S01E02 - Hyde Park Corner.en'\n",
      " 'Crown, The S01E03 - Windsor.en.FORCED'\n",
      " 'Crown, The S01E03 - Windsor.en.SDH' 'Crown, The S01E03 - Windsor.en'\n",
      " 'Crown, The S01E04 - Act of God.en.SDH'\n",
      " 'Crown, The S01E04 - Act of God.en'\n",
      " 'Crown, The S01E05 - Smoke and Mirrors.en.FORCED'\n",
      " 'Crown, The S01E05 - Smoke and Mirrors.en.SDH'\n",
      " 'Crown, The S01E05 - Smoke and Mirrors.en'\n",
      " 'Crown, The S01E06 - Gelignite.en.SDH' 'Crown, The S01E06 - Gelignite.en'\n",
      " 'Crown, The S01E07 - Scientia Potentia Est.en.FORCED'\n",
      " 'Crown, The S01E07 - Scientia Potentia Est.en.SDH'\n",
      " 'Crown, The S01E07 - Scientia Potentia Est.en'\n",
      " 'Crown, The S01E08 - Pride & Joy.en.SDH'\n",
      " 'Crown, The S01E08 - Pride & Joy.en'\n",
      " 'Crown, The S01E09 - Assassins.en.SDH' 'Crown, The S01E09 - Assassins.en'\n",
      " 'Crown, The S01E10 - Gloriana.en.FORCED'\n",
      " 'Crown, The S01E10 - Gloriana.en.SDH' 'Crown, The S01E10 - Gloriana.en'\n",
      " 'Frozen.2013.WEB-DL.DSNP' 'icarus.2017.web.x264-strife'\n",
      " 'SOMM.Into.the.Bottle.2015.1080p.BluRay.x265-RARBG.en'\n",
      " 'The.Grinch.2018.REMUX.1080p.Blu-ray.AVC.TrueHD.DTS-HD.MA.7.1-LEGi0N.English'\n",
      " 'The.Sound.of.Music.1965.WEBRip.iTunes'\n",
      " 'The.True.Cost.2015.BluRay.720p.700MB.Ganool.com'\n",
      " 'Virgin.River.S01E01.INTERNAL.720p.WEB.x264-STRiFE'\n",
      " 'Virgin.River.S01E02.INTERNAL.720p.WEB.x264-STRiFE'\n",
      " 'Virgin.River.S01E03.INTERNAL.720p.WEB.x264-STRiFE'\n",
      " 'Virgin.River.S01E04.INTERNAL.720p.WEB.x264-STRiFE'\n",
      " 'Virgin.River.S01E05.INTERNAL.720p.WEB.x264-STRiFE'\n",
      " 'Virgin.River.S01E06.INTERNAL.720p.WEB.x264-STRiFE'\n",
      " 'Virgin.River.S01E07.INTERNAL.720p.WEB.x264-STRiFE'\n",
      " 'Virgin.River.S01E08.INTERNAL.720p.WEB.x264-STRiFE'\n",
      " 'Virgin.River.S01E09.INTERNAL.720p.WEB.x264-STRiFE'\n",
      " 'Virgin.River.S01E10.INTERNAL.720p.WEB.x264-STRiFE' '.DS_Store'\n",
      " 'Breaking_Bad_The_Movie(2017)'\n",
      " 'BrenВ.Brown.The.Call.to.Courage.2019.720.NF.720p.DDP.5.1.x264-CafeFlix'\n",
      " 'Casper' 'Gogo_Loves_English'\n",
      " 'Harry_Potter_and_the_philosophers_stone(2001)' 'Pride_and_Prejudice'\n",
      " 'The_Ghost_Writer' 'Up(2009)' 'Westworld_scenes_of_Dr_Robert_Ford'] \n",
      "Всего таких фильмов: 51\n",
      "\n",
      "Массив уникальных названий, для которых есть уровень, но нет субтитров: \n",
      " ['The Secret Life of Pets.en' 'Up (2009)'\n",
      " 'SOMM.Into.the.Bottle.2015.1080p.BluRay.x265-RARBG.en.srt' 'Glass Onion'\n",
      " 'Matilda(2022)' 'Bullet train' 'Thor: love and thunder' 'Lightyear'\n",
      " 'The Grinch'] \n",
      "Всего таких фильмов: 9\n"
     ]
    }
   ],
   "source": [
    "# у нас есть 2 столбца хранящих названия: name и movie\n",
    "# надо определить, почему столбцы не смёржились. Чем вызвана такая ошибка?\n",
    "movie_loss = df_full[df_full['movie'].isna()]\n",
    "print('\\nМассив уникальных названий, которые есть в таблице df: \\n',\n",
    "      movie_loss['name'].values, \n",
    "     '\\nВсего таких фильмов:', len(movie_loss['name'].values))\n",
    "\n",
    "name_loss = df_full[df_full['name'].isna()]\n",
    "print('\\nМассив уникальных названий, для которых есть уровень, но нет субтитров: \\n',\n",
    "      name_loss['movie'].values,\n",
    "     '\\nВсего таких фильмов:', len(name_loss['movie'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>level</th>\n",
       "      <th>name_to_merge</th>\n",
       "      <th>label</th>\n",
       "      <th>name</th>\n",
       "      <th>subtitle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>The Secret Life of Pets.en</td>\n",
       "      <td>B2</td>\n",
       "      <td>the_secret_life_of_petsen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Up (2009)</td>\n",
       "      <td>A2/A2+</td>\n",
       "      <td>up_2009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>SOMM.Into.the.Bottle.2015.1080p.BluRay.x265-RA...</td>\n",
       "      <td>B2</td>\n",
       "      <td>sommintothebottle20151080pblurayx265rarbgensrt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>Glass Onion</td>\n",
       "      <td>B2</td>\n",
       "      <td>glass_onion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>Matilda(2022)</td>\n",
       "      <td>C1</td>\n",
       "      <td>matilda2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Bullet train</td>\n",
       "      <td>B1</td>\n",
       "      <td>bullet_train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Thor: love and thunder</td>\n",
       "      <td>B2</td>\n",
       "      <td>thor_love_and_thunder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>Lightyear</td>\n",
       "      <td>B2</td>\n",
       "      <td>lightyear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>The Grinch</td>\n",
       "      <td>B1</td>\n",
       "      <td>the_grinch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 movie   level  \\\n",
       "82                          The Secret Life of Pets.en      B2   \n",
       "106                                          Up (2009)  A2/A2+   \n",
       "155  SOMM.Into.the.Bottle.2015.1080p.BluRay.x265-RA...      B2   \n",
       "235                                        Glass Onion      B2   \n",
       "236                                      Matilda(2022)      C1   \n",
       "237                                       Bullet train      B1   \n",
       "238                             Thor: love and thunder      B2   \n",
       "239                                          Lightyear      B2   \n",
       "240                                         The Grinch      B1   \n",
       "\n",
       "                                      name_to_merge label name subtitle  \n",
       "82                        the_secret_life_of_petsen   NaN  NaN      NaN  \n",
       "106                                         up_2009   NaN  NaN      NaN  \n",
       "155  sommintothebottle20151080pblurayx265rarbgensrt   NaN  NaN      NaN  \n",
       "235                                     glass_onion   NaN  NaN      NaN  \n",
       "236                                     matilda2022   NaN  NaN      NaN  \n",
       "237                                    bullet_train   NaN  NaN      NaN  \n",
       "238                           thor_love_and_thunder   NaN  NaN      NaN  \n",
       "239                                       lightyear   NaN  NaN      NaN  \n",
       "240                                      the_grinch   NaN  NaN      NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movie                                                          NaN\n",
       "level                                                          NaN\n",
       "name_to_merge          sommintothebottle20151080pblurayx265rarbgen\n",
       "label                                                           B2\n",
       "name             SOMM.Into.the.Bottle.2015.1080p.BluRay.x265-RA...\n",
       "subtitle         What-- What is a sommelier?\\nA sommelier is a ...\n",
       "Name: 268, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.loc[268] # можно заметить, что объект 155, на самом делел представлен в наших данных, под идексом 266\n",
    "# поскольку уровень совпадает, индекс 155 можно удалить.\n",
    "# Все остальные значения уникальные. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df_full.drop(index=155, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В результате можно видеть, что у нас есть 8 фильмов, у которых уже присвоена метка класса, но отсуствуют субтитры. Эти субтитры можно догрузить. Но мы удалим этим субтитры. Также удалим пустые субтитры**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_with_miss_values = df_full[df_full['subtitle'].isin([''])].index.to_list() + df_full[df_full['subtitle'].isna()].index.to_list()\n",
    "df_full = df_full.drop(index=index_with_miss_values, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full['level'].fillna(df_full['label'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A2</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2/A2+</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2/A2+, B1</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B1</th>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B1, B2</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B2</th>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C1</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subtitles</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count\n",
       "level            \n",
       "A2              6\n",
       "A2/A2+         25\n",
       "A2/A2+, B1      5\n",
       "B1             54\n",
       "B1, B2          8\n",
       "B2            136\n",
       "C1             23\n",
       "Subtitles       9"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# изучим категоризацию объектов\n",
    "df_full.reset_index().groupby(['level']).index.agg({'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что есть метки со смешенные классов. Правильно будет сделать так:\n",
    "    \n",
    "    1) Для А2/A2+, - можно перенести объекты к метке A2, поскольку их мало. Таким образом мы снизим дисбаланс классов\n",
    "    2) Для меток A2/A2+, B1, мы можем отнести объект к метке B1.\n",
    "    3) Для метки B1, B2, можем отнести объект к B1.\n",
    " \n",
    "Такими действиями мы сократим дисбаланс классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A2</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B1</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B2</th>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C1</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subtitles</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count\n",
       "level           \n",
       "A2            31\n",
       "B1            67\n",
       "B2           136\n",
       "C1            23\n",
       "Subtitles      9"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full['level'].replace({'A2/A2+': 'A2',\n",
    "                          'A2/A2+, B1': 'B1',\n",
    "                          'B1, B2': 'B1'},\n",
    "                         inplace=True)\n",
    "\n",
    "df_full.reset_index().groupby(['level']).index.agg({'count'}) # дисбаланс классов заметно сократился"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 фильмов - не имеют разметки, при этом, наблюдается явный дисбаланс классов.\n",
    "решить эту задачу можно несколькими методами:\n",
    "    \n",
    "    1) сгенерировать тексты определнного уровня;\n",
    "    2) тонко настроить модель, чтобы она учитывала баланс классов;\n",
    "    \n",
    "Поскольку время выполнения проекта сильно ограничено, будем использовать второй вариант. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# там где отсутсвует название name, перенесём его из названия movie\n",
    "df_full['name'].fillna(df_full['movie'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# теперь создадим объект, который будем в дальнейшем использовать для анализа \n",
    "data = df_full[['name', 'subtitle', 'level']]\n",
    "data = data.astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед проведением eda нам необходимо очитсить наши тексты. для этого можно использовать несколько технологий:\n",
    "\n",
    "    1) Стоит очистить тексты от лишних символов, признаков разметки;\n",
    "    2) затем нужно провести токенизацию текста;\n",
    "    2) очистить текст от предлогов;\n",
    "    4) далее стоит провести pos-теггинг и леммматизацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaning:\n",
    "    '''\n",
    "    Класс предназначен для предобработки текста\n",
    "    '''\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "\n",
    "    def process_text(self):\n",
    "        '''\n",
    "        В субтитрах представленно много \"служебных\" данных, они представлены в разных видах скобок. Это надо очистить. \n",
    "        '''\n",
    "        # Удаление объектов в квадратных скобках и самих скобок\n",
    "        text = re.sub(r'\\[.*?\\]', '', self.text)\n",
    "\n",
    "        # Удаление объектов в фигурных скобках и самих скобок\n",
    "        text = re.sub(r'\\{.*?\\}', '', text)\n",
    "\n",
    "        # Удаление объектов внутри скобок\n",
    "        text = re.sub(r'\\(.*?\\)', '', text)\n",
    "\n",
    "        # Удаление объектов между <>\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "        # Удаление лишних пробелов и символов перевода строки\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "\n",
    "        self.text = text\n",
    "\n",
    "    def tokenizer(self):\n",
    "        '''\n",
    "        Токенизация текста позволит провести все остальные операции с текстом\n",
    "        '''\n",
    "        tokens = word_tokenize(self.text)\n",
    "        self.text = tokens\n",
    "\n",
    "    def drop_stopwords(self):\n",
    "        '''\n",
    "        Удаление стоп-слов позволит повысить качество анализа\n",
    "        '''\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_tokens = [word for word in self.text if word.lower() not in stop_words]\n",
    "        self.text = filtered_tokens\n",
    "\n",
    "    def pos_tagging(self):\n",
    "        '''\n",
    "        Пос-теггинг поможет, поскольку в наших словарях есть части речи. это повысит точность коннекта\n",
    "        '''\n",
    "        pos_tags = nltk.pos_tag(self.text)\n",
    "        self.text = pos_tags\n",
    "\n",
    "    def lemmatization(self):\n",
    "        '''\n",
    "        Функция предназначена для лемматизации текста\n",
    "        '''\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_words = [\n",
    "            (lemmatizer.lemmatize(word, pos=self.get_wordnet_pos(tag)), tag)\n",
    "            for word, tag in self.text\n",
    "        ]\n",
    "        self.text = lemmatized_words\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_wordnet_pos(tag):\n",
    "        '''\n",
    "        Вспомогательная функция служит для того, чтобы в зависимости от тега вернуть нужную лемму\n",
    "        '''\n",
    "        if tag.startswith('J'):\n",
    "            return nltk.corpus.wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return nltk.corpus.wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return nltk.corpus.wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return nltk.corpus.wordnet.ADV\n",
    "        else:\n",
    "            return nltk.corpus.wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('-', ':'), ('-', ':'), ('Little', 'JJ'), ('girl', 'NN'), ('?', '.'), (\"'m\", 'VBP'), ('policeman', 'JJ'), ('.', '.'), ('Little', 'JJ'), ('girl', 'NN'), ('.', '.'), (\"n't\", 'RB'), ('afraid', 'JJ'), (',', ','), ('okay', 'VB'), ('?', '.'), ('Little', 'JJ'), ('girl', 'NN'), ('.', '.'), ('Oh', 'NNP'), ('God', 'NNP'), ('.', '.'), ('-', ':'), ('-', ':'), ('Man', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "text = df.loc[0]['subtitle']\n",
    "\n",
    "cleaner = TextCleaning(text)\n",
    "cleaner.process_text()\n",
    "cleaner.tokenizer()\n",
    "cleaner.drop_stopwords()\n",
    "cleaner.pos_tagging()\n",
    "cleaner.lemmatization()\n",
    "\n",
    "print(cleaner.text[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 40.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# теперь применим наш класс к нашему дата-фрейму:\n",
    "def clean(x):\n",
    "    cleaner = TextCleaning(x)\n",
    "    cleaner.process_text()\n",
    "    cleaner.tokenizer()\n",
    "    cleaner.drop_stopwords()\n",
    "    cleaner.pos_tagging()\n",
    "    cleaner.lemmatization()\n",
    "    return cleaner.text\n",
    "\n",
    "data['sub_clean'] = data.subtitle.apply(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На выходе, мы получаем список, состоящий из кортежей, где в первом элементе кортежа представлена лема, а во втором элементе кортежа представлен тегг.\n",
    "\n",
    "Далее предлагается написать ещё одну функцию, которая будет распаршивать резюмирующий объект, и возвращать несколько списков:\n",
    "    \n",
    "    1) Количество слов;\n",
    "    2) Количество слов относящихся к A1, A2, B1, B2, C1, C2, кол-во слов не представленных в словаре;\n",
    "    3) Количество теггов, относящихся к A1, A2, B1, B2, C1, C2, кол-во слов не представленных в словаре;\n",
    "    4) Количество знаков.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим словарь с приблизительными пос-теггами по уровню английского\n",
    "pos_tegg_dict = {\n",
    "    'a1': ['NN', 'NNS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ'],\n",
    "    'a2': ['DT', 'PRP', 'RB'],\n",
    "    'b1': ['CC', 'IN', 'MD', 'WRB'],\n",
    "    'b2': ['RP', 'JJR'],\n",
    "    'c1': ['JJS'],\n",
    "    'c2': ['FW']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tags(input_dataframe: pd.DataFrame,\n",
    "               column: str,\n",
    "               tag_dict: dict,\n",
    "               word_dict: dict) -> pd.DataFrame:\n",
    "    tag_columns = ['a1_teg', 'a2_teg', 'b1_teg', 'b2_teg', 'c1_teg', 'c2_teg']\n",
    "    word_columns = ['a1_word', 'a2_word', 'b1_word', 'b2_word', 'c1_word', 'c2_word']\n",
    "\n",
    "    input_dataframe = input_dataframe.reset_index(drop=True)\n",
    "\n",
    "    inversed_tag_dict = {value: key for key, lst in tag_dict.items() for value in lst}\n",
    "    inversed_word_dict = {value: key for key, lst in word_dict.items() for value in lst}\n",
    "\n",
    "    row_dict = {}\n",
    "\n",
    "    for i, list_pair in enumerate(input_dataframe[column]):\n",
    "        tag_counters = {}.fromkeys(tag_dict.keys(), 0)\n",
    "        word_counters = {}.fromkeys(inversed_word_dict.keys(), 0)\n",
    "        word_counters |= {'words_without_tag': 0, 'non-words': 0}\n",
    "        for word, tag in list_pair:\n",
    "            word_ = word.lower()\n",
    "            if inversed_tag_dict.get(tag):\n",
    "                tag_counters[inversed_tag_dict[tag]] += 1\n",
    "            if inversed_word_dict.get(word_):\n",
    "                word_counters[inversed_word_dict[word_]] += 1\n",
    "            else:\n",
    "                # print(word_)\n",
    "                # non words:\n",
    "                non_word = re.search(r'\\W+', word_)\n",
    "                if non_word is not None:\n",
    "                    word_counters['non-words'] += 1\n",
    "                    # print(non_word.group())\n",
    "                else:\n",
    "                    i_think_it_is_word = re.search(r'\\w+', word_)\n",
    "                    if i_think_it_is_word:\n",
    "                        word_counters['words_without_tag'] += 1\n",
    "        else:\n",
    "            row_dict |= {i: [\n",
    "                {f'{k}_tag': v for k, v in tag_counters.items()},\n",
    "                {f'{k}_word': v for k, v in word_counters.items() if v != 0}\n",
    "            ]}\n",
    "    new_columns = []\n",
    "    for val in row_dict.values():\n",
    "        new_dict = {}\n",
    "        for d in val:\n",
    "            new_dict |= {**d}\n",
    "        new_columns.append(new_dict)\n",
    "    return pd.concat([input_dataframe, pd.DataFrame(new_columns)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = count_tags(data, 'sub_clean', pos_tegg_dict, cefr_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# распарсим данные\n",
    "data['sub'] = [' '.join([v[0].lower() for v in value if v[0] not in '/.,<>!*&^%($#)@_!:?....-.-']) for value in data['sub_clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['name', 'subtitle', 'sub_clean'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Подготовка данных и моделирование**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_test_valid_split(data):\n",
    "    '''\n",
    "    Функция предназначена для разделения данных на test, train и valid;\n",
    "    Функция нужна для реперезентативности выборки обучения. Учитывает дисбаланс классов;\n",
    "    '''\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    data = data.reset_index()\n",
    "    \n",
    "    test = data[data['level'] == 'Subtitles'].reset_index(drop=True)\n",
    "    data = data[data['level'] != 'Subtitles']\n",
    "    \n",
    "    stats = data.groupby('level')['level'].agg({'count'}).reset_index()\n",
    "    stats['target_count_indexes'] = (stats['count'] * 0.8).astype('int')\n",
    "    \n",
    "    d = {} # создадим словари со всеми индексами\n",
    "    for level in data['level'].unique():\n",
    "        d.update({level: []})\n",
    "    \n",
    "    for level, idx in zip(data['level'], data['index']):\n",
    "        d[level].append(idx)\n",
    "    \n",
    "    d_count = {} # словарь для отбора кол-ва индексов, которые нужно отобрать для каждой группы\n",
    "    \n",
    "    for level, count in zip(stats['level'], stats['target_count_indexes']):\n",
    "        d_count.update({level: count})\n",
    "    \n",
    "    idx_lists = list() # создадим список с индексами\n",
    "    for key, n in d_count.items(): # распарсим словарь с количеством\n",
    "        lists_means = random.sample(d[key], n) # сделаем рандомный выбор значений, для каждого ключа, для n - кол-ва\n",
    "        idx_lists.append(lists_means) # добавим в конец списка индексы\n",
    "        \n",
    "    index_target = list()\n",
    "    for values in idx_lists:\n",
    "        for v in values:\n",
    "            index_target.append(v)\n",
    "    \n",
    "    train = data[data['index'].isin(index_target)].reset_index(drop=True)\n",
    "    valid = data[~data['index'].isin(index_target)].reset_index(drop=True)\n",
    "    \n",
    "    train.rename(columns={'level': 'label_class'}, inplace=True)\n",
    "    test.rename(columns={'level': 'label_class'}, inplace=True)\n",
    "    valid.rename(columns={'level': 'label_class'}, inplace=True)\n",
    "    \n",
    "    return train.drop(columns='index', axis=1), test.drop(columns='index', axis=1), valid.drop(columns='index', axis=1)\n",
    "\n",
    "train, test, valid = train_test_valid_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тренировочная выборка: 76 %\n",
      "Валидационная выборка: 20 %\n",
      "Тестовая выборка: 3 %\n"
     ]
    }
   ],
   "source": [
    "print('Тренировочная выборка:', round((len(train)/len(data))*100), '%')\n",
    "print('Валидационная выборка:', round((len(valid)/len(data))*100), '%')\n",
    "print('Тестовая выборка:', round((len(test)/len(data))*100), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.3813014\ttotal: 266ms\tremaining: 6m 38s\n",
      "100:\tlearn: 1.0145394\ttotal: 11.3s\tremaining: 2m 35s\n",
      "200:\tlearn: 0.7956311\ttotal: 22.2s\tremaining: 2m 23s\n",
      "300:\tlearn: 0.6532248\ttotal: 33.2s\tremaining: 2m 12s\n",
      "400:\tlearn: 0.5431636\ttotal: 44.2s\tremaining: 2m 1s\n",
      "500:\tlearn: 0.4546312\ttotal: 55.2s\tremaining: 1m 50s\n",
      "600:\tlearn: 0.3737064\ttotal: 1m 6s\tremaining: 1m 38s\n",
      "700:\tlearn: 0.3006440\ttotal: 1m 16s\tremaining: 1m 27s\n",
      "800:\tlearn: 0.2413312\ttotal: 1m 27s\tremaining: 1m 16s\n",
      "900:\tlearn: 0.1971673\ttotal: 1m 38s\tremaining: 1m 5s\n",
      "1000:\tlearn: 0.1652286\ttotal: 1m 49s\tremaining: 54.7s\n",
      "1100:\tlearn: 0.1399913\ttotal: 2m\tremaining: 43.7s\n",
      "1200:\tlearn: 0.1205215\ttotal: 2m 11s\tremaining: 32.8s\n",
      "1300:\tlearn: 0.1057819\ttotal: 2m 22s\tremaining: 21.8s\n",
      "1400:\tlearn: 0.0939148\ttotal: 2m 33s\tremaining: 10.8s\n",
      "1499:\tlearn: 0.0841683\ttotal: 2m 44s\tremaining: 0us\n",
      "0:\tlearn: 1.3822102\ttotal: 141ms\tremaining: 3m 31s\n",
      "100:\tlearn: 1.0594788\ttotal: 12.7s\tremaining: 2m 55s\n",
      "200:\tlearn: 0.8798502\ttotal: 25.1s\tremaining: 2m 42s\n",
      "300:\tlearn: 0.7495456\ttotal: 37.5s\tremaining: 2m 29s\n",
      "400:\tlearn: 0.6412767\ttotal: 49.9s\tremaining: 2m 16s\n",
      "500:\tlearn: 0.5531525\ttotal: 1m 2s\tremaining: 2m 4s\n",
      "600:\tlearn: 0.4669978\ttotal: 1m 14s\tremaining: 1m 51s\n",
      "700:\tlearn: 0.3875147\ttotal: 1m 27s\tremaining: 1m 39s\n",
      "800:\tlearn: 0.3146978\ttotal: 1m 40s\tremaining: 1m 27s\n",
      "900:\tlearn: 0.2600717\ttotal: 1m 53s\tremaining: 1m 15s\n",
      "1000:\tlearn: 0.2190439\ttotal: 2m 7s\tremaining: 1m 3s\n",
      "1100:\tlearn: 0.1867759\ttotal: 2m 20s\tremaining: 51.1s\n",
      "1200:\tlearn: 0.1610096\ttotal: 2m 34s\tremaining: 38.5s\n",
      "1300:\tlearn: 0.1410460\ttotal: 2m 48s\tremaining: 25.8s\n",
      "1400:\tlearn: 0.1244845\ttotal: 3m 2s\tremaining: 12.9s\n",
      "1499:\tlearn: 0.1113520\ttotal: 3m 15s\tremaining: 0us\n",
      "0:\tlearn: 1.3807074\ttotal: 160ms\tremaining: 4m\n",
      "100:\tlearn: 1.0174662\ttotal: 14.1s\tremaining: 3m 15s\n",
      "200:\tlearn: 0.8252555\ttotal: 28.8s\tremaining: 3m 5s\n",
      "300:\tlearn: 0.6939273\ttotal: 43s\tremaining: 2m 51s\n",
      "400:\tlearn: 0.5966176\ttotal: 57.2s\tremaining: 2m 36s\n",
      "500:\tlearn: 0.5136836\ttotal: 1m 11s\tremaining: 2m 21s\n",
      "600:\tlearn: 0.4404319\ttotal: 1m 25s\tremaining: 2m 7s\n",
      "700:\tlearn: 0.3680481\ttotal: 1m 39s\tremaining: 1m 53s\n",
      "800:\tlearn: 0.3066261\ttotal: 1m 53s\tremaining: 1m 39s\n",
      "900:\tlearn: 0.2570285\ttotal: 2m 8s\tremaining: 1m 25s\n",
      "1000:\tlearn: 0.2188700\ttotal: 2m 22s\tremaining: 1m 11s\n",
      "1100:\tlearn: 0.1904019\ttotal: 2m 37s\tremaining: 56.9s\n",
      "1200:\tlearn: 0.1675428\ttotal: 2m 51s\tremaining: 42.7s\n",
      "1300:\tlearn: 0.1488681\ttotal: 3m 6s\tremaining: 28.5s\n",
      "1400:\tlearn: 0.1332751\ttotal: 3m 20s\tremaining: 14.2s\n",
      "1499:\tlearn: 0.1210393\ttotal: 3m 34s\tremaining: 0us\n",
      "0:\tlearn: 1.3411384\ttotal: 118ms\tremaining: 2m 56s\n",
      "100:\tlearn: 0.1646425\ttotal: 17.9s\tremaining: 4m 7s\n",
      "200:\tlearn: 0.0532029\ttotal: 36.1s\tremaining: 3m 53s\n",
      "300:\tlearn: 0.0286277\ttotal: 54.3s\tremaining: 3m 36s\n",
      "400:\tlearn: 0.0191924\ttotal: 1m 12s\tremaining: 3m 18s\n",
      "500:\tlearn: 0.0144493\ttotal: 1m 29s\tremaining: 2m 58s\n",
      "600:\tlearn: 0.0114624\ttotal: 1m 46s\tremaining: 2m 39s\n",
      "700:\tlearn: 0.0094312\ttotal: 2m 4s\tremaining: 2m 21s\n",
      "800:\tlearn: 0.0081052\ttotal: 2m 21s\tremaining: 2m 3s\n",
      "900:\tlearn: 0.0070036\ttotal: 2m 39s\tremaining: 1m 45s\n",
      "1000:\tlearn: 0.0061784\ttotal: 2m 56s\tremaining: 1m 28s\n",
      "1100:\tlearn: 0.0055240\ttotal: 3m 14s\tremaining: 1m 10s\n",
      "1200:\tlearn: 0.0050110\ttotal: 3m 32s\tremaining: 52.9s\n",
      "1300:\tlearn: 0.0045724\ttotal: 3m 51s\tremaining: 35.4s\n",
      "1400:\tlearn: 0.0042036\ttotal: 4m 9s\tremaining: 17.6s\n",
      "1499:\tlearn: 0.0038895\ttotal: 4m 26s\tremaining: 0us\n",
      "0:\tlearn: 1.3488116\ttotal: 138ms\tremaining: 3m 26s\n",
      "100:\tlearn: 0.2241353\ttotal: 13s\tremaining: 3m\n",
      "200:\tlearn: 0.0712669\ttotal: 25.7s\tremaining: 2m 46s\n",
      "300:\tlearn: 0.0377051\ttotal: 38.4s\tremaining: 2m 33s\n",
      "400:\tlearn: 0.0248246\ttotal: 50.9s\tremaining: 2m 19s\n",
      "500:\tlearn: 0.0183324\ttotal: 1m 3s\tremaining: 2m 6s\n",
      "600:\tlearn: 0.0144531\ttotal: 1m 16s\tremaining: 1m 54s\n",
      "700:\tlearn: 0.0118150\ttotal: 1m 29s\tremaining: 1m 41s\n",
      "800:\tlearn: 0.0099760\ttotal: 1m 41s\tremaining: 1m 28s\n",
      "900:\tlearn: 0.0086409\ttotal: 1m 53s\tremaining: 1m 15s\n",
      "1000:\tlearn: 0.0076322\ttotal: 2m 6s\tremaining: 1m 3s\n",
      "1100:\tlearn: 0.0067825\ttotal: 2m 19s\tremaining: 50.6s\n",
      "1200:\tlearn: 0.0061055\ttotal: 2m 32s\tremaining: 38s\n",
      "1300:\tlearn: 0.0055888\ttotal: 2m 45s\tremaining: 25.3s\n",
      "1400:\tlearn: 0.0051182\ttotal: 2m 58s\tremaining: 12.6s\n",
      "1499:\tlearn: 0.0047373\ttotal: 3m 10s\tremaining: 0us\n",
      "0:\tlearn: 1.3513656\ttotal: 131ms\tremaining: 3m 15s\n",
      "100:\tlearn: 0.2170475\ttotal: 12s\tremaining: 2m 46s\n",
      "200:\tlearn: 0.0848372\ttotal: 23.8s\tremaining: 2m 33s\n",
      "300:\tlearn: 0.0544510\ttotal: 35.4s\tremaining: 2m 21s\n",
      "400:\tlearn: 0.0425955\ttotal: 47.7s\tremaining: 2m 10s\n",
      "500:\tlearn: 0.0364332\ttotal: 60s\tremaining: 1m 59s\n",
      "600:\tlearn: 0.0316285\ttotal: 1m 12s\tremaining: 1m 47s\n",
      "700:\tlearn: 0.0286692\ttotal: 1m 24s\tremaining: 1m 36s\n",
      "800:\tlearn: 0.0263294\ttotal: 1m 36s\tremaining: 1m 23s\n",
      "900:\tlearn: 0.0244034\ttotal: 1m 47s\tremaining: 1m 11s\n",
      "1000:\tlearn: 0.0231523\ttotal: 1m 59s\tremaining: 59.6s\n",
      "1100:\tlearn: 0.0220959\ttotal: 2m 11s\tremaining: 47.6s\n",
      "1200:\tlearn: 0.0212392\ttotal: 2m 22s\tremaining: 35.6s\n",
      "1300:\tlearn: 0.0204370\ttotal: 2m 34s\tremaining: 23.7s\n",
      "1400:\tlearn: 0.0197512\ttotal: 2m 46s\tremaining: 11.8s\n",
      "1499:\tlearn: 0.0192043\ttotal: 2m 58s\tremaining: 0us\n",
      "0:\tlearn: 1.3629987\ttotal: 357ms\tremaining: 2m 58s\n",
      "100:\tlearn: 0.2355281\ttotal: 35.2s\tremaining: 2m 18s\n",
      "200:\tlearn: 0.0820122\ttotal: 1m 9s\tremaining: 1m 43s\n",
      "300:\tlearn: 0.0457716\ttotal: 1m 44s\tremaining: 1m 9s\n",
      "400:\tlearn: 0.0311221\ttotal: 2m 19s\tremaining: 34.4s\n",
      "499:\tlearn: 0.0239646\ttotal: 2m 54s\tremaining: 0us\n",
      "0:\tlearn: 1.3608639\ttotal: 344ms\tremaining: 2m 51s\n",
      "100:\tlearn: 0.2848675\ttotal: 38.7s\tremaining: 2m 32s\n",
      "200:\tlearn: 0.0990644\ttotal: 1m 17s\tremaining: 1m 54s\n",
      "300:\tlearn: 0.0537100\ttotal: 1m 56s\tremaining: 1m 16s\n",
      "400:\tlearn: 0.0363643\ttotal: 2m 34s\tremaining: 38.2s\n",
      "499:\tlearn: 0.0271986\ttotal: 3m 12s\tremaining: 0us\n",
      "0:\tlearn: 1.3468654\ttotal: 374ms\tremaining: 3m 6s\n",
      "100:\tlearn: 0.2972194\ttotal: 41.2s\tremaining: 2m 42s\n",
      "200:\tlearn: 0.1185684\ttotal: 1m 19s\tremaining: 1m 58s\n",
      "300:\tlearn: 0.0745377\ttotal: 1m 55s\tremaining: 1m 16s\n",
      "400:\tlearn: 0.0564559\ttotal: 2m 32s\tremaining: 37.8s\n",
      "499:\tlearn: 0.0463288\ttotal: 3m 9s\tremaining: 0us\n",
      "0:\tlearn: 1.3239528\ttotal: 61.7ms\tremaining: 1m 32s\n",
      "100:\tlearn: 0.1278165\ttotal: 3.97s\tremaining: 54.9s\n",
      "200:\tlearn: 0.0310697\ttotal: 7.94s\tremaining: 51.3s\n",
      "300:\tlearn: 0.0147960\ttotal: 11.8s\tremaining: 46.9s\n",
      "400:\tlearn: 0.0091793\ttotal: 15.5s\tremaining: 42.5s\n",
      "500:\tlearn: 0.0066758\ttotal: 19.2s\tremaining: 38.4s\n",
      "600:\tlearn: 0.0051328\ttotal: 23.1s\tremaining: 34.6s\n",
      "700:\tlearn: 0.0042047\ttotal: 26.9s\tremaining: 30.7s\n",
      "800:\tlearn: 0.0035440\ttotal: 30.7s\tremaining: 26.8s\n",
      "900:\tlearn: 0.0030525\ttotal: 34.4s\tremaining: 22.9s\n",
      "1000:\tlearn: 0.0026698\ttotal: 38.2s\tremaining: 19.1s\n",
      "1100:\tlearn: 0.0023596\ttotal: 42s\tremaining: 15.2s\n",
      "1200:\tlearn: 0.0021259\ttotal: 45.8s\tremaining: 11.4s\n",
      "1300:\tlearn: 0.0019317\ttotal: 49.6s\tremaining: 7.58s\n",
      "1400:\tlearn: 0.0017593\ttotal: 53.4s\tremaining: 3.77s\n",
      "1499:\tlearn: 0.0016247\ttotal: 57.1s\tremaining: 0us\n",
      "0:\tlearn: 1.3322253\ttotal: 51.1ms\tremaining: 1m 16s\n",
      "100:\tlearn: 0.1708805\ttotal: 4.39s\tremaining: 1m\n",
      "200:\tlearn: 0.0440942\ttotal: 8.75s\tremaining: 56.6s\n",
      "300:\tlearn: 0.0203505\ttotal: 13.1s\tremaining: 52.1s\n",
      "400:\tlearn: 0.0120905\ttotal: 17.4s\tremaining: 47.7s\n",
      "500:\tlearn: 0.0086290\ttotal: 21.8s\tremaining: 43.4s\n",
      "600:\tlearn: 0.0065970\ttotal: 26.1s\tremaining: 39s\n",
      "700:\tlearn: 0.0054245\ttotal: 30.4s\tremaining: 34.6s\n",
      "800:\tlearn: 0.0045340\ttotal: 34.7s\tremaining: 30.3s\n",
      "900:\tlearn: 0.0038838\ttotal: 39s\tremaining: 26s\n",
      "1000:\tlearn: 0.0033850\ttotal: 43.3s\tremaining: 21.6s\n",
      "1100:\tlearn: 0.0029989\ttotal: 47.7s\tremaining: 17.3s\n",
      "1200:\tlearn: 0.0026937\ttotal: 52.1s\tremaining: 13s\n",
      "1300:\tlearn: 0.0024539\ttotal: 56.4s\tremaining: 8.63s\n",
      "1400:\tlearn: 0.0022512\ttotal: 1m\tremaining: 4.29s\n",
      "1499:\tlearn: 0.0020747\ttotal: 1m 5s\tremaining: 0us\n",
      "0:\tlearn: 1.3327622\ttotal: 51.3ms\tremaining: 1m 16s\n",
      "100:\tlearn: 0.1796831\ttotal: 4.55s\tremaining: 1m 3s\n",
      "200:\tlearn: 0.0591584\ttotal: 9.05s\tremaining: 58.5s\n",
      "300:\tlearn: 0.0361873\ttotal: 13.6s\tremaining: 54s\n",
      "400:\tlearn: 0.0272431\ttotal: 18.1s\tremaining: 49.5s\n",
      "500:\tlearn: 0.0231961\ttotal: 22.6s\tremaining: 45.1s\n",
      "600:\tlearn: 0.0203756\ttotal: 27.1s\tremaining: 40.6s\n",
      "700:\tlearn: 0.0186534\ttotal: 31.6s\tremaining: 36s\n",
      "800:\tlearn: 0.0174697\ttotal: 36.1s\tremaining: 31.5s\n",
      "900:\tlearn: 0.0165059\ttotal: 40.6s\tremaining: 27s\n",
      "1000:\tlearn: 0.0157127\ttotal: 45.1s\tremaining: 22.5s\n",
      "1100:\tlearn: 0.0151414\ttotal: 49.6s\tremaining: 18s\n",
      "1200:\tlearn: 0.0146628\ttotal: 54.2s\tremaining: 13.5s\n",
      "1300:\tlearn: 0.0142886\ttotal: 58.7s\tremaining: 8.98s\n",
      "1400:\tlearn: 0.0139751\ttotal: 1m 3s\tremaining: 4.48s\n",
      "1499:\tlearn: 0.0136898\ttotal: 1m 8s\tremaining: 0us\n",
      "0:\tlearn: 1.3659428\ttotal: 93.3ms\tremaining: 2m 19s\n",
      "100:\tlearn: 0.5642634\ttotal: 9s\tremaining: 2m 4s\n",
      "200:\tlearn: 0.2382879\ttotal: 17.7s\tremaining: 1m 54s\n",
      "300:\tlearn: 0.1339138\ttotal: 26.6s\tremaining: 1m 45s\n",
      "400:\tlearn: 0.0894614\ttotal: 35.4s\tremaining: 1m 37s\n",
      "500:\tlearn: 0.0651026\ttotal: 44.3s\tremaining: 1m 28s\n",
      "600:\tlearn: 0.0497684\ttotal: 52.9s\tremaining: 1m 19s\n",
      "700:\tlearn: 0.0405422\ttotal: 1m 1s\tremaining: 1m 10s\n",
      "800:\tlearn: 0.0337819\ttotal: 1m 10s\tremaining: 1m 1s\n",
      "900:\tlearn: 0.0288803\ttotal: 1m 19s\tremaining: 53s\n",
      "1000:\tlearn: 0.0252070\ttotal: 1m 28s\tremaining: 44.3s\n",
      "1100:\tlearn: 0.0223981\ttotal: 1m 37s\tremaining: 35.5s\n",
      "1200:\tlearn: 0.0201024\ttotal: 1m 46s\tremaining: 26.5s\n",
      "1300:\tlearn: 0.0182580\ttotal: 1m 55s\tremaining: 17.6s\n",
      "1400:\tlearn: 0.0166141\ttotal: 2m 3s\tremaining: 8.75s\n",
      "1499:\tlearn: 0.0152816\ttotal: 2m 12s\tremaining: 0us\n",
      "0:\tlearn: 1.3707924\ttotal: 108ms\tremaining: 2m 42s\n",
      "100:\tlearn: 0.6470996\ttotal: 15.5s\tremaining: 3m 35s\n",
      "200:\tlearn: 0.3137336\ttotal: 31.4s\tremaining: 3m 22s\n",
      "300:\tlearn: 0.1732872\ttotal: 47.4s\tremaining: 3m 8s\n",
      "400:\tlearn: 0.1139537\ttotal: 1m 3s\tremaining: 2m 53s\n",
      "500:\tlearn: 0.0811843\ttotal: 1m 19s\tremaining: 2m 37s\n",
      "600:\tlearn: 0.0623167\ttotal: 1m 34s\tremaining: 2m 21s\n",
      "700:\tlearn: 0.0498019\ttotal: 1m 50s\tremaining: 2m 5s\n",
      "800:\tlearn: 0.0414876\ttotal: 2m 6s\tremaining: 1m 50s\n",
      "900:\tlearn: 0.0354786\ttotal: 2m 22s\tremaining: 1m 34s\n",
      "1000:\tlearn: 0.0309282\ttotal: 2m 38s\tremaining: 1m 19s\n",
      "1100:\tlearn: 0.0272593\ttotal: 2m 55s\tremaining: 1m 3s\n",
      "1200:\tlearn: 0.0242887\ttotal: 3m 11s\tremaining: 47.7s\n",
      "1300:\tlearn: 0.0220042\ttotal: 3m 29s\tremaining: 32.1s\n",
      "1400:\tlearn: 0.0200243\ttotal: 3m 46s\tremaining: 16s\n",
      "1499:\tlearn: 0.0183275\ttotal: 4m 5s\tremaining: 0us\n",
      "0:\tlearn: 1.3694754\ttotal: 135ms\tremaining: 3m 22s\n",
      "100:\tlearn: 0.6088573\ttotal: 18s\tremaining: 4m 8s\n",
      "200:\tlearn: 0.3135397\ttotal: 35.6s\tremaining: 3m 50s\n",
      "300:\tlearn: 0.1897864\ttotal: 52.9s\tremaining: 3m 30s\n",
      "400:\tlearn: 0.1320466\ttotal: 1m 10s\tremaining: 3m 12s\n",
      "500:\tlearn: 0.1017066\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "600:\tlearn: 0.0822282\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "700:\tlearn: 0.0705060\ttotal: 2m 3s\tremaining: 2m 20s\n",
      "800:\tlearn: 0.0620440\ttotal: 2m 21s\tremaining: 2m 3s\n",
      "900:\tlearn: 0.0560970\ttotal: 2m 38s\tremaining: 1m 45s\n",
      "1000:\tlearn: 0.0515062\ttotal: 2m 56s\tremaining: 1m 27s\n",
      "1100:\tlearn: 0.0476208\ttotal: 3m 13s\tremaining: 1m 10s\n",
      "1200:\tlearn: 0.0441171\ttotal: 3m 31s\tremaining: 52.6s\n",
      "1300:\tlearn: 0.0415649\ttotal: 3m 50s\tremaining: 35.2s\n",
      "1400:\tlearn: 0.0393394\ttotal: 4m 7s\tremaining: 17.5s\n",
      "1499:\tlearn: 0.0377133\ttotal: 4m 24s\tremaining: 0us\n",
      "0:\tlearn: 1.3509596\ttotal: 608ms\tremaining: 5m 3s\n",
      "100:\tlearn: 0.2934614\ttotal: 1m 2s\tremaining: 4m 8s\n",
      "200:\tlearn: 0.1204614\ttotal: 2m 2s\tremaining: 3m 1s\n",
      "300:\tlearn: 0.0736983\ttotal: 3m 3s\tremaining: 2m 1s\n",
      "400:\tlearn: 0.0552057\ttotal: 4m 4s\tremaining: 1m\n",
      "499:\tlearn: 0.0456839\ttotal: 5m 5s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "# подготовим колонки\n",
    "text_column = 'sub'\n",
    "target = 'label_class'\n",
    "numeric_columns = [column for column in train.columns if column not in text_column and column not in target]\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "le = LabelEncoder()\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "train[numeric_columns] = mms.fit_transform(train[numeric_columns])\n",
    "valid[numeric_columns] = mms.transform(valid[numeric_columns])\n",
    "test[numeric_columns] = mms.transform(test[numeric_columns])\n",
    "\n",
    "train[target] = le.fit_transform(train[target])\n",
    "valid[target] = le.transform(valid[target])\n",
    "\n",
    "tfidf_matrix = tf_idf.fit_transform(train[text_column])\n",
    "tfidf_df_train = pd.DataFrame(tfidf_matrix.toarray(), columns=tf_idf.get_feature_names())\n",
    "\n",
    "tfidf_matrix = tf_idf.transform(valid[text_column])\n",
    "tfidf_df_valid = pd.DataFrame(tfidf_matrix.toarray(), columns=tf_idf.get_feature_names())\n",
    "\n",
    "tfidf_matrix = tf_idf.transform(test[text_column])\n",
    "tfidf_df_test = pd.DataFrame(tfidf_matrix.toarray(), columns=tf_idf.get_feature_names())\n",
    "\n",
    "train = train.drop(columns='sub', axis=1)\n",
    "train = pd.concat([train, tfidf_df_train], axis=1)\n",
    "\n",
    "valid = valid.drop(columns='sub', axis=1)\n",
    "valid = pd.concat([valid, tfidf_df_valid], axis=1)\n",
    "\n",
    "test = test.drop(columns='sub', axis=1)\n",
    "test = pd.concat([test, tfidf_df_test], axis=1)\n",
    "\n",
    "target_train = train[target]\n",
    "target_valid = valid[target]\n",
    "\n",
    "features_train = train.drop(columns=target, axis=1).fillna(0)\n",
    "features_valid = valid.drop(columns=target, axis=1).fillna(0)\n",
    "features_test = test.drop(columns=target, axis=1).fillna(0)\n",
    "\n",
    "\n",
    "# Создание pipeline без параметров модели\n",
    "pipeline = Pipeline([\n",
    "    ('model', CatBoostClassifier())\n",
    "])\n",
    "\n",
    "# Задание сетки параметров для случайного поиска\n",
    "parameters = {\n",
    "    'model__iterations': [500, 1000, 1500],\n",
    "    'model__loss_function': ['MultiClass', 'Logloss'],\n",
    "    'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'model__depth': [4, 6, 8],\n",
    "    'model__l2_leaf_reg': [1, 3, 5],\n",
    "    'model__border_count': [32, 64, 128],\n",
    "    'model__verbose': [100]\n",
    "}\n",
    "\n",
    "# Поиск наилучших параметров с использованием RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(pipeline, parameters, cv=3, n_iter=10, random_state=42)\n",
    "random_search.fit(features_train, target_train)\n",
    "\n",
    "# Получение наилучшей модели\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "\n",
    "# Предсказание с использованием лучшей модели\n",
    "y_pred = best_model.predict(features_valid)\n",
    "\n",
    "# Оценка точности предсказаний\n",
    "accuracy = accuracy_score(target_valid, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Резюмирующая точность оказалась 66% - это может говорить о трёх проблемах:\n",
    "    \n",
    "    1) Плохо выполнен процесс предобработки, подготовки данных;\n",
    "    2) Данные описаны неточно (стоит провести дополнительную обработку размерности входящего массива, проработать веса);\n",
    "    3) Данных недостаточно для обучения (стоит расширить дата-сет).\n",
    "    \n",
    "    \n",
    "После ревью планируется:\n",
    "    \n",
    "    1) Исправить ошибки, если они есть;\n",
    "    2) попробовать другие подходы к подготовке (например, сделать оценку частнотности, по уникальным словам, а не по всем или провести лемматизацию слов, представленных в словаре, для повышения точности соединения)\n",
    "    3) расширить данные;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Параметры дающие accuracy 66%**\n",
    "\n",
    "    {\n",
    "    'memory': None,\n",
    "     'steps': [('model', <catboost.core.CatBoostClassifier at 0x21616165fd0>)],\n",
    "     'verbose': False,\n",
    "     'model': <catboost.core.CatBoostClassifier at 0x21616165fd0>,\n",
    "     'model__iterations': 500,\n",
    "     'model__learning_rate': 0.1,\n",
    "     'model__depth': 8,\n",
    "     'model__l2_leaf_reg': 5,\n",
    "     'model__loss_function': 'MultiClass',\n",
    "     'model__border_count': 128,\n",
    "     'model__verbose': 100\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
